{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ltfidf с нормализацией.ipynb","provenance":[],"authorship_tag":"ABX9TyO8GpsSPybUB+KG0bV6iKiP"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"wcJItX1wafD0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"4301bbed-72ae-450a-90d0-724c575b4d23","executionInfo":{"status":"ok","timestamp":1580420842049,"user_tz":-180,"elapsed":95359,"user":{"displayName":"Ростислав Андреевич Корст","photoUrl":"","userId":"17024356270045662756"}}},"source":["!git clone https://github.com/Samsung-IT-Academy/stepik-dl-nlp.git && pip install -r stepik-dl-nlp/requirements.txt\n","import sys; sys.path.append('/content/stepik-dl-nlp')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Cloning into 'stepik-dl-nlp'...\n","remote: Enumerating objects: 26, done.\u001b[K\n","remote: Counting objects: 100% (26/26), done.\u001b[K\n","remote: Compressing objects: 100% (22/22), done.\u001b[K\n","remote: Total 216 (delta 10), reused 18 (delta 4), pack-reused 190\u001b[K\n","Receiving objects: 100% (216/216), 42.11 MiB | 18.20 MiB/s, done.\n","Resolving deltas: 100% (89/89), done.\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 1)) (0.22.1)\n","Collecting spacy-udpipe\n","  Downloading https://files.pythonhosted.org/packages/53/ea/dc89025c7f0ed8e6d7fd11893e53eddb53ecb20c017f8ffba4a11eea64ef/spacy_udpipe-0.1.0-py3-none-any.whl\n","Collecting pymorphy2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n","\u001b[K     |████████████████████████████████| 51kB 4.8MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.2 in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 4)) (1.3.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 5)) (3.1.2)\n","Collecting ipymarkup\n","  Downloading https://files.pythonhosted.org/packages/d8/29/eaa1bcf649d6333dea829c05577c67f881d0555b6d77c1da72afda5c847d/ipymarkup-0.5.0-py2.py3-none-any.whl\n","Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 7)) (4.2.6)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 8)) (1.4.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 9)) (0.25.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 10)) (4.28.1)\n","Collecting youtokentome\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/16/4cb7a9358430996bd6fa7daf32421105fe37a7bd0e4da1f79496e15aa509/youtokentome-1.0.5-cp36-cp36m-manylinux2010_x86_64.whl (1.7MB)\n","\u001b[K     |████████████████████████████████| 1.7MB 5.4MB/s \n","\u001b[?25hRequirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 12)) (0.9.0)\n","Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 13)) (4.6.1)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 14)) (5.5.0)\n","Collecting pyconll\n","  Downloading https://files.pythonhosted.org/packages/2c/6e/c325d0db05ac1b8d45645de903e4ba691d419e861c915c3d4ebfcaf8ac25/pyconll-2.2.1-py3-none-any.whl\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r stepik-dl-nlp/requirements.txt (line 1)) (0.14.1)\n","Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r stepik-dl-nlp/requirements.txt (line 1)) (1.17.5)\n","Requirement already satisfied: spacy>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.1.9)\n","Collecting ufal.udpipe>=1.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/72/2b8b9dc7c80017c790bb3308bbad34b57accfed2ac2f1f4ab252ff4e9cb2/ufal.udpipe-1.2.0.3.tar.gz (304kB)\n","\u001b[K     |████████████████████████████████| 307kB 46.5MB/s \n","\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2->-r stepik-dl-nlp/requirements.txt (line 3)) (0.6.2)\n","Collecting dawg-python>=0.7\n","  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n","Collecting pymorphy2-dicts<3.0,>=2.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n","\u001b[K     |████████████████████████████████| 7.1MB 26.6MB/s \n","\u001b[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (1.1.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (2.6.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (2.4.6)\n","Requirement already satisfied: intervaltree==2.1.0 in /usr/local/lib/python3.6/dist-packages (from ipymarkup->-r stepik-dl-nlp/requirements.txt (line 6)) (2.1.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r stepik-dl-nlp/requirements.txt (line 9)) (2018.9)\n","Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from youtokentome->-r stepik-dl-nlp/requirements.txt (line 11)) (7.0)\n","Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (4.5.3)\n","Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (5.3.4)\n","Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (4.3.3)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.7.5)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.8.1)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (1.0.18)\n","Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (4.7.0)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (2.1.3)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (42.0.2)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (4.4.1)\n","Requirement already satisfied: requests>=2.21 in /usr/local/lib/python3.6/dist-packages (from pyconll->-r stepik-dl-nlp/requirements.txt (line 15)) (2.21.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.6.0)\n","Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (7.0.8)\n","Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.2.4)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.0.3)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.0.2)\n","Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.9.6)\n","Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.0.1)\n","Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.0.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (1.12.0)\n","Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.6/dist-packages (from intervaltree==2.1.0->ipymarkup->-r stepik-dl-nlp/requirements.txt (line 6)) (2.1.0)\n","Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (17.0.0)\n","Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (4.6.1)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.1.0->ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (0.2.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.1.8)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.6.0)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.21->pyconll->-r stepik-dl-nlp/requirements.txt (line 15)) (2.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.21->pyconll->-r stepik-dl-nlp/requirements.txt (line 15)) (2019.11.28)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.21->pyconll->-r stepik-dl-nlp/requirements.txt (line 15)) (3.0.4)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.21->pyconll->-r stepik-dl-nlp/requirements.txt (line 15)) (1.24.3)\n","Building wheels for collected packages: ufal.udpipe\n","  Building wheel for ufal.udpipe (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ufal.udpipe: filename=ufal.udpipe-1.2.0.3-cp36-cp36m-linux_x86_64.whl size=5625771 sha256=b77e03af8ae332e789af2397af94ef18ee699cd29d448217e7913a5723f6094f\n","  Stored in directory: /root/.cache/pip/wheels/0c/9d/db/6d3404c33da5b7adb6c6972853efb6a27649d3ba15f7e9bebb\n","Successfully built ufal.udpipe\n","Installing collected packages: ufal.udpipe, spacy-udpipe, dawg-python, pymorphy2-dicts, pymorphy2, ipymarkup, youtokentome, pyconll\n","Successfully installed dawg-python-0.7.2 ipymarkup-0.5.0 pyconll-2.2.1 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985 spacy-udpipe-0.1.0 ufal.udpipe-1.2.0.3 youtokentome-1.0.5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nmoIr286arLA","colab_type":"code","colab":{}},"source":["import warnings\n","warnings.filterwarnings('ignore')\n","\n","from sklearn.datasets import fetch_20newsgroups\n","from sklearn.metrics import accuracy_score\n","\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import collections\n","\n","import torch\n","from torch import nn\n","from torch.nn import functional as F\n","\n","import dlnlputils\n","from dlnlputils.data import tokenize_text_simple_regex, tokenize_corpus, build_vocabulary, \\\n","    vectorize_texts, SparseFeaturesDataset\n","from dlnlputils.pipeline import train_eval_loop, predict_with_model, init_random_seed\n","\n","init_random_seed()\n","\n","import random\n","\n","random.seed(0)\n","np.random.seed(0)\n","torch.manual_seed(0)\n","torch.cuda.manual_seed(0)\n","torch.backends.cudnn.deterministic = True"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QL73h5AKaxYt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":177},"outputId":"d0437907-8775-45ab-9b39-46835884a360","executionInfo":{"status":"ok","timestamp":1580425836529,"user_tz":-180,"elapsed":1107,"user":{"displayName":"Ростислав Андреевич Корст","photoUrl":"","userId":"17024356270045662756"}}},"source":["text = [\"Казнить нельзя, помиловать. Нельзя наказывать.\", \"Казнить, нельзя помиловать. Нельзя освободить.\", \"Нельзя не помиловать.\", \"Обязательно освободить.\"]\n","\n","tokenized_list_of_docs = tokenize_corpus(text, min_token_size=1)\n","\n","print(tokenized_list_of_docs)\n","\n","word_id = build_vocabulary(tokenized_list_of_docs, max_doc_freq=1, min_count=1)\n","print(word_id)\n","print()\n","# уже отсортировано по алфавиту и по частоте\n","word_id = list(zip(list(word_id[0].keys()), list(word_id[1])))\n","print(word_id)\n","print()\n","word_id = sorted(word_id, key=lambda x: (x[1], x[0]))\n","print(word_id)\n","print()\n","\n","print(' '.join(list(map(lambda x: x[0], word_id))))\n","print(' '.join(list(map(lambda x: str(x[1]), word_id))))\n","#\"\"\"print(' '.join(list(word_id[0].keys())))\n","#print(list(word_id[1]))\n","#a = list(word_id[1])\n","#a = list(map(lambda x: str(x), a))\n","#print(' '.join(a))\"\"\"\n"],"execution_count":19,"outputs":[{"output_type":"stream","text":["[['казнить', 'нельзя', 'помиловать', 'нельзя', 'наказывать'], ['казнить', 'нельзя', 'помиловать', 'нельзя', 'освободить'], ['нельзя', 'не', 'помиловать'], ['обязательно', 'освободить']]\n","({'помиловать': 0, 'нельзя': 1, 'казнить': 2, 'освободить': 3, 'наказывать': 4, 'не': 5, 'обязательно': 6}, array([0.75, 0.75, 0.5 , 0.5 , 0.25, 0.25, 0.25], dtype=float32))\n","\n","[('помиловать', 0.75), ('нельзя', 0.75), ('казнить', 0.5), ('освободить', 0.5), ('наказывать', 0.25), ('не', 0.25), ('обязательно', 0.25)]\n","\n","[('наказывать', 0.25), ('не', 0.25), ('обязательно', 0.25), ('казнить', 0.5), ('освободить', 0.5), ('нельзя', 0.75), ('помиловать', 0.75)]\n","\n","наказывать не обязательно казнить освободить нельзя помиловать\n","0.25 0.25 0.25 0.5 0.5 0.75 0.75\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Gk1HYvkgcW8l","colab_type":"code","colab":{}},"source":["import numpy as np\n","import scipy.sparse\n","import torch\n","from torch.utils.data import Dataset\n","\n","\n","def vectorize_texts(tokenized_texts, word2id, word2freq, mode='tfidf', scale=True):\n","    assert mode in {'tfidf', 'idf', 'tf', 'bin', 'ltfidf'}\n","\n","    # считаем количество употреблений каждого слова в каждом документе\n","    result = scipy.sparse.dok_matrix((len(tokenized_texts), len(word2id)), dtype='float32')\n","    for text_i, text in enumerate(tokenized_texts):\n","        for token in text:\n","            if token in word2id:\n","                result[text_i, word2id[token]] += 1\n","\n","    # получаем бинарные вектора \"встречается или нет\"\n","    if mode == 'bin':\n","        result = (result > 0).astype('float32')\n","\n","    # получаем вектора относительных частот слова в документе\n","    elif mode == 'tf':\n","        result = result.tocsr()\n","        result = result.multiply(1 / result.sum(1))\n","\n","    # полностью убираем информацию о количестве употреблений слова в данном документе,\n","    # но оставляем информацию о частотности слова в корпусе в целом\n","    elif mode == 'idf':\n","        result = (result > 0).astype('float32').multiply(1 / word2freq)\n","\n","    # учитываем всю информацию, которая у нас есть:\n","    # частоту слова в документе и частоту слова в корпусе\n","    elif mode == 'tfidf':\n","        result = result.tocsr()\n","        result = result.multiply(1 / result.sum(1))  # разделить каждую строку на её длину\n","        result = result.multiply(1 / word2freq)  # разделить каждый столбец на вес слова\n","\n","    elif mode == 'ltfidf':\n","        result = result.tocsr()\n","        result = result.multiply(1 / result.sum(1))  # разделить каждую строку на её длину\n","        result = scipy.sparse.csr_matrix.toarray(scipy.sparse.dok_matrix.tocsr(result))\n","        result = np.log(result + 1)\n","        result = scipy.sparse.csr_matrix(result)\n","        result = result.multiply(1 / word2freq)  # разделить каждый столбец на вес слова\n","\n","    if scale:\n","        result = result.tocsc()\n","        #result -= result.min()\n","        #result /= (result.max() + 1e-6)\n","\n","        result = scipy.sparse.csr_matrix.toarray(scipy.sparse.dok_matrix.tocsr(result))\n","        std = result.std(0, ddof=1)\n","        mean = result.mean(0)\n","\n","        result = (result - mean) / std\n","        result = scipy.sparse.csr_matrix(result)\n","\n","\n","\n","    return result.tocsr()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HcK3jLQ3sCRE","colab_type":"code","colab":{}},"source":["train_tokenized = tokenize_corpus(text, min_token_size=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VVesMf0puZkg","colab_type":"code","colab":{}},"source":["def build_vocabulary2(tokenized_texts, max_size=1000000, max_doc_freq=0.8, min_count=5, pad_word=None):\n","    word_counts = collections.defaultdict(int)\n","    # количество документов, в которых встречается данное слово\n","    doc_n = 0\n","\n","    # посчитать количество документов, в которых употребляется каждое слово\n","    # а также общее количество документов\n","    for txt in tokenized_texts:\n","        doc_n += 1\n","        unique_text_tokens = set(txt)\n","        for token in unique_text_tokens:\n","            word_counts[token] += 1\n","\n","    # убрать слишком редкие и слишком частые слова\n","    word_counts = {word: cnt for word, cnt in word_counts.items()\n","                   if cnt >= min_count and cnt / doc_n <= max_doc_freq}\n","\n","    # отсортировать слова по убыванию частоты\n","    sorted_word_counts = sorted(word_counts.items(),\n","                                key=lambda pair: (pair[1], pair[0]))\n","\n","    # добавим несуществующее слово с индексом 0 для удобства пакетной обработки\n","    if pad_word is not None:\n","        sorted_word_counts = [(pad_word, 0)] + sorted_word_counts\n","\n","    # если у нас по прежнему слишком много слов, оставить только max_size самых частотных\n","    if len(word_counts) > max_size:\n","        sorted_word_counts = sorted_word_counts[:max_size]\n","\n","    # нумеруем слова\n","    word2id = {word: i for i, (word, _) in enumerate(sorted_word_counts)}\n","\n","    # нормируем частоты слов\n","    word2freq = np.array([cnt / doc_n for _, cnt in sorted_word_counts], dtype='float32')\n","\n","    return word2id, word2freq"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kOKVHmnxsK1U","colab_type":"code","colab":{}},"source":["vocabulary, word_doc_freq = build_vocabulary2(train_tokenized, max_doc_freq=1, min_count=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_oRvZ0X0gIe7","colab_type":"code","colab":{}},"source":["VECTORIZATION_MODE = 'ltfidf'\n","train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8ItbYC2mugfA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"68d4880a-d313-4692-f491-eda63769543d","executionInfo":{"status":"ok","timestamp":1580425947890,"user_tz":-180,"elapsed":833,"user":{"displayName":"Ростислав Андреевич Корст","photoUrl":"","userId":"17024356270045662756"}}},"source":["train_vectors"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<4x7 sparse matrix of type '<class 'numpy.float32'>'\n","\twith 28 stored elements in Compressed Sparse Row format>"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"FrLCLGEeuyIE","colab_type":"code","colab":{}},"source":["result = scipy.sparse.csr_matrix.toarray(scipy.sparse.dok_matrix.tocsr(train_vectors))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J0-0ZrF_u1w5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":159},"outputId":"62afd798-fb77-4232-808b-27e2da78b21a","executionInfo":{"status":"ok","timestamp":1580426060140,"user_tz":-180,"elapsed":1091,"user":{"displayName":"Ростислав Андреевич Корст","photoUrl":"","userId":"17024356270045662756"}}},"source":["list(result)"],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[array([ 1.5000001 , -0.5       , -0.5       ,  0.8660254 , -0.76301265,\n","         0.59546685,  0.16096792], dtype=float32),\n"," array([-0.50000006, -0.5       , -0.5       ,  0.8660254 ,  0.18368238,\n","         0.59546685,  0.16096792], dtype=float32),\n"," array([-0.50000006,  1.5       , -0.5       , -0.8660254 , -0.76301265,\n","         0.2938242 ,  1.042435  ], dtype=float32),\n"," array([-0.50000006, -0.5       ,  1.5       , -0.8660254 ,  1.342343  ,\n","        -1.4847579 , -1.364371  ], dtype=float32)]"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"HEtKWaB-vBZ9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":159},"outputId":"b6291480-7301-4ac5-f8e7-d763ef8f2ea3","executionInfo":{"status":"ok","timestamp":1580426234910,"user_tz":-180,"elapsed":1072,"user":{"displayName":"Ростислав Андреевич Корст","photoUrl":"","userId":"17024356270045662756"}}},"source":["for doc in result:\n","    print(' '.join(list(map(str, list(doc)))))\n","    print()"],"execution_count":29,"outputs":[{"output_type":"stream","text":["1.5000001 -0.5 -0.5 0.8660254 -0.76301265 0.59546685 0.16096792\n","\n","-0.50000006 -0.5 -0.5 0.8660254 0.18368238 0.59546685 0.16096792\n","\n","-0.50000006 1.5 -0.5 -0.8660254 -0.76301265 0.2938242 1.042435\n","\n","-0.50000006 -0.5 1.5 -0.8660254 1.342343 -1.4847579 -1.364371\n","\n"],"name":"stdout"}]}]}